import 'dart:async';
import 'dart:io';
import 'dart:math';
import 'package:path/path.dart' as path;

// Core imports
import 'cache/cache_manager.dart';
import 'cache/lru_cache.dart';
import 'cache/file_cache.dart';
import 'cache/cached_model_provider.dart';

import 'jobs/job_queue.dart';
import 'jobs/worker_pool.dart';
import 'jobs/job_persistence.dart';

import 'models/model_interfaces.dart';
import 'models/providers/openai_provider.dart';

/// Comprehensive example demonstrating the Performance Optimization system
Future<void> runPerformanceOptimizationExample() async {
  print('üöÄ Starting Performance Optimization System Example');
  print('=' * 60);

  try {
    // 1. Initialize cache system
    await _demonstrateCacheSystem();
    
    print('\n' + '=' * 60);
    
    // 2. Demonstrate background job processing
    await _demonstrateJobProcessing();
    
    print('\n' + '=' * 60);
    
    // 3. Show cached model provider
    await _demonstrateCachedModelProvider();
    
    print('\n' + '=' * 60);
    
    // 4. Performance benchmarks
    await _runPerformanceBenchmarks();
    
  } catch (e, stackTrace) {
    print('‚ùå Example failed: $e');
    print('Stack trace: $stackTrace');
  }
  
  print('\n‚úÖ Performance Optimization System Example Completed!');
}

/// Demonstrate the multi-level cache system
Future<void> _demonstrateCacheSystem() async {
  print('üì¶ CACHE SYSTEM DEMONSTRATION');
  print('-' * 30);
  
  // Initialize cache manager
  final cacheDir = Directory(path.join(Directory.current.path, 'cache'));
  final fileCache = FileCache(directory: cacheDir);
  await fileCache.initialize();
  
  final cacheManager = CacheManager(
    config: CacheManagerConfig(
      memoryMaxSize: 50,
      enableRedis: false, // Disabled for example
    ),
  );
  await cacheManager.initialize();
  
  print('üîß Cache manager initialized');
  
  // Test cache operations
  print('\nüìù Testing cache operations...');
  
  // Test different cache levels
  final testData = {
    'user_profile': {'id': 1, 'name': 'John Doe', 'email': 'john@example.com'},
    'api_response': {'status': 200, 'data': List.generate(100, (i) => 'item_$i')},
    'large_dataset': List.generate(1000, (i) => {'index': i, 'value': Random().nextDouble()}),
  };
  
  final stopwatch = Stopwatch()..start();
  
  // Store data with different TTLs
  await cacheManager.put('user_profile', testData['user_profile'], 
      ttl: const Duration(hours: 1), level: CacheLevel.memory);
  await cacheManager.put('api_response', testData['api_response'], 
      ttl: const Duration(minutes: 30), level: CacheLevel.all);
  await cacheManager.put('large_dataset', testData['large_dataset'], 
      ttl: const Duration(hours: 6), level: CacheLevel.disk);
  
  stopwatch.stop();
  print('‚úÖ Cached 3 items in ${stopwatch.elapsedMilliseconds}ms');
  
  // Test cache hits
  stopwatch.reset()..start();
  
  final userProfile = await cacheManager.get<Map<String, dynamic>>('user_profile');
  final apiResponse = await cacheManager.get<Map<String, dynamic>>('api_response');
  final largeDataset = await cacheManager.get<List<Map<String, dynamic>>>('large_dataset');
  
  stopwatch.stop();
  print('‚úÖ Retrieved 3 items in ${stopwatch.elapsedMilliseconds}ms');
  
  // Verify data integrity
  assert(userProfile != null && userProfile['name'] == 'John Doe');
  assert(apiResponse != null && apiResponse['status'] == 200);
  assert(largeDataset != null && largeDataset.length == 1000);
  
  print('‚úÖ Data integrity verified');
  
  // Show cache statistics
  final stats = await cacheManager.getStatistics();
  print('\nüìä Cache Statistics:');
  print('Memory cache: ${stats['memory_cache']}');
  print('Disk cache: ${stats['disk_cache']}');
  print('Overall hit rate: ${(stats['overall_hit_rate'] * 100).toStringAsFixed(1)}%');
  
  await cacheManager.dispose();
}

/// Demonstrate background job processing with worker pool
Future<void> _demonstrateJobProcessing() async {
  print('‚öôÔ∏è JOB PROCESSING DEMONSTRATION');
  print('-' * 30);
  
  // Initialize job persistence
  final cacheDir = Directory(path.join(Directory.current.path, 'jobs'));
  final fileCache = FileCache(directory: cacheDir);
  await fileCache.initialize();
  
  final persistenceManager = JobPersistenceManager(
    cache: fileCache,
    checkpointDirectory: path.join(cacheDir.path, 'checkpoints'),
    enableWAL: true,
  );
  await persistenceManager.initialize();
  
  // Initialize worker pool
  final workerConfig = WorkerPoolConfig(
    minWorkers: 2,
    maxWorkers: 4,
    enableAutoScaling: true,
  );
  
  final workerPool = WorkerPool(config: workerConfig);
  await workerPool.start();
  
  // Initialize job queue
  final jobQueue = JobQueue(
    maxConcurrentJobs: 4,
    persistenceCache: fileCache,
    enablePersistence: true,
  );
  
  print('üîß Job processing system initialized');
  
  // Create test jobs
  final jobs = <Job>[
    DocumentProcessingJob(
      id: 'doc_1',
      data: {'file_path': '/documents/report.pdf', 'processing_type': 'extract_text'},
      priority: JobPriority.high,
    ),
    ModelEmbeddingJob(
      id: 'embed_1',
      data: {'text': 'This is a sample text for embedding generation', 'model': 'text-embedding-ada-002'},
      priority: JobPriority.normal,
    ),
    FileIndexingJob(
      id: 'index_1',
      data: {'directory_path': '/data/documents', 'include_patterns': ['*.pdf', '*.docx']},
      priority: JobPriority.low,
    ),
    DocumentProcessingJob(
      id: 'doc_2',
      data: {'file_path': '/documents/manual.docx', 'processing_type': 'extract_metadata'},
      priority: JobPriority.normal,
    ),
  ];
  
  print('\nüìã Adding ${jobs.length} jobs to queue...');
  
  // Add jobs to queue
  for (final job in jobs) {
    await jobQueue.addJob(job);
    await persistenceManager.persistJob(
      QueuedJob(job: job, createdAt: DateTime.now()),
      JobPersistenceState.queued,
    );
  }
  
  // Listen to job events
  int completedJobs = 0;
  final jobCompleter = Completer<void>();
  
  jobQueue.onJobCompleted.listen((result) {
    completedJobs++;
    print('‚úÖ Job completed: ${result.jobId} (success: ${result.success})');
    
    if (result.success) {
      persistenceManager.persistJob(
        QueuedJob(job: jobs.firstWhere((j) => j.id == result.jobId), createdAt: DateTime.now()),
        JobPersistenceState.completed,
      );
    }
    
    if (completedJobs == jobs.length) {
      jobCompleter.complete();
    }
  });
  
  workerPool.onEvent.listen((event) {
    if (event is JobAssignedEvent) {
      print('üéØ Job assigned to worker ${event.worker.id}: ${event.job.job.id}');
    } else if (event is WorkerCreatedEvent) {
      print('üë∑ Created worker: ${event.worker.id}');
    }
  });
  
  print('‚è≥ Processing jobs...');
  final processingStart = DateTime.now();
  
  // Process jobs with worker pool
  final availableJobsFuture = _processJobsWithWorkerPool(jobQueue, workerPool);
  
  // Wait for all jobs to complete
  await Future.any([
    jobCompleter.future,
    Future.delayed(const Duration(seconds: 60)), // Timeout
  ]);
  
  final processingTime = DateTime.now().difference(processingStart);
  print('‚è±Ô∏è All jobs processed in ${processingTime.inSeconds} seconds');
  
  // Show statistics
  final queueStats = jobQueue.getStatistics();
  final workerStats = workerPool.getStatistics();
  final persistenceStats = persistenceManager.getStatistics();
  
  print('\nüìä Processing Statistics:');
  print('Queue: $queueStats');
  print('Workers: $workerStats');
  print('Persistence: $persistenceStats');
  
  // Test recovery
  print('\nüîÑ Testing job recovery...');
  final recoveryResult = await persistenceManager.recoverJobs();
  print('Recovery result: $recoveryResult');
  
  // Cleanup
  await jobQueue.dispose();
  await workerPool.stop();
  await persistenceManager.dispose();
}

/// Process jobs with worker pool
Future<void> _processJobsWithWorkerPool(JobQueue jobQueue, WorkerPool workerPool) async {
  // Simulate job processing by assigning jobs from queue to workers
  while (true) {
    // In a real implementation, this would be handled by the job queue manager
    // For demo purposes, we just wait for the jobs to complete
    await Future.delayed(const Duration(seconds: 1));
    
    final stats = jobQueue.getStatistics();
    if (stats.runningJobs == 0 && stats.pendingJobs == 0) {
      break;
    }
  }
}

/// Demonstrate cached model provider
Future<void> _demonstrateCachedModelProvider() async {
  print('ü§ñ CACHED MODEL PROVIDER DEMONSTRATION');
  print('-' * 30);
  
  // Create cache manager
  final cacheDir = Directory(path.join(Directory.current.path, 'model_cache'));
  final fileCache = FileCache(directory: cacheDir);
  await fileCache.initialize();
  
  final cacheManager = CacheManager(
    config: CacheManagerConfig(
      memoryMaxSize: 100,
      enableRedis: false,
    ),
  );
  await cacheManager.initialize();
  
  // Create mock model provider
  final baseProvider = MockOpenAIProvider();
  
  // Wrap with caching
  final cachedProvider = CachedModelProvider(
    baseProvider,
    cacheManager,
    defaultCacheTTL: const Duration(hours: 2),
    enableStreaming: false,
    maxTokensToCache: 4000,
  );
  
  print('üîß Cached model provider initialized');
  
  // Test requests
  final testRequests = [
    ModelRequest(
      model: 'gpt-3.5-turbo',
      messages: [Message.user('What is the capital of France?')],
      maxTokens: 100,
      temperature: 0.1, // Low temperature for deterministic responses
    ),
    ModelRequest(
      model: 'gpt-3.5-turbo',
      messages: [Message.user('Explain quantum computing in simple terms')],
      maxTokens: 200,
      temperature: 0.2,
    ),
    ModelRequest(
      model: 'gpt-3.5-turbo',
      messages: [Message.user('What is the capital of France?')], // Same as first - should hit cache
      maxTokens: 100,
      temperature: 0.1,
    ),
  ];
  
  print('\nüì§ Sending ${testRequests.length} requests...');
  
  final responses = <ModelResponse>[];
  final stopwatch = Stopwatch();
  
  for (int i = 0; i < testRequests.length; i++) {
    stopwatch.reset()..start();
    
    final response = await cachedProvider.complete(testRequests[i]);
    responses.add(response);
    
    stopwatch.stop();
    print('‚úÖ Request ${i + 1} completed in ${stopwatch.elapsedMilliseconds}ms');
  }
  
  // Verify cache hit for duplicate request
  print('\nüéØ Cache Performance:');
  final health = await cachedProvider.checkHealth();
  final cacheStats = health.metadata['cache_stats'] as Map<String, dynamic>?;
  
  if (cacheStats != null) {
    print('Hit rate: ${(cacheStats['provider_hit_rate'] * 100).toStringAsFixed(1)}%');
    print('Cache hits: ${cacheStats['provider_cache_hits']}');
    print('Cache misses: ${cacheStats['provider_cache_misses']}');
  }
  
  // Cleanup
  await cacheManager.dispose();
}

/// Run performance benchmarks
Future<void> _runPerformanceBenchmarks() async {
  print('‚ö° PERFORMANCE BENCHMARKS');
  print('-' * 30);
  
  // Benchmark cache performance
  await _benchmarkCachePerformance();
  
  print('');
  
  // Benchmark job processing
  await _benchmarkJobProcessing();
}

/// Benchmark cache performance
Future<void> _benchmarkCachePerformance() async {
  print('üèÉ Cache Performance Benchmark');
  
  final cacheDir = Directory(path.join(Directory.current.path, 'benchmark_cache'));
  final fileCache = FileCache(directory: cacheDir);
  await fileCache.initialize();
  
  final cacheManager = CacheManager(
    config: CacheManagerConfig(
      memoryMaxSize: 1000,
      enableRedis: false,
    ),
  );
  await cacheManager.initialize();
  
  const iterations = 1000;
  final testData = {'value': Random().nextInt(1000000)};
  
  // Benchmark write performance
  final writeStopwatch = Stopwatch()..start();
  
  for (int i = 0; i < iterations; i++) {
    await cacheManager.put('benchmark_key_$i', testData, 
        level: CacheLevel.memory);
  }
  
  writeStopwatch.stop();
  final writeRate = (iterations / writeStopwatch.elapsed.inMilliseconds * 1000).round();
  print('‚úÖ Write performance: $writeRate ops/sec');
  
  // Benchmark read performance
  final readStopwatch = Stopwatch()..start();
  
  for (int i = 0; i < iterations; i++) {
    final result = await cacheManager.get('benchmark_key_$i');
    assert(result != null);
  }
  
  readStopwatch.stop();
  final readRate = (iterations / readStopwatch.elapsed.inMilliseconds * 1000).round();
  print('‚úÖ Read performance: $readRate ops/sec');
  
  // Show final statistics
  final stats = await cacheManager.getStatistics();
  print('üìä Final cache hit rate: ${(stats['overall_hit_rate'] * 100).toStringAsFixed(1)}%');
  
  await cacheManager.dispose();
}

/// Benchmark job processing performance
Future<void> _benchmarkJobProcessing() async {
  print('üèÉ Job Processing Benchmark');
  
  final cacheDir = Directory(path.join(Directory.current.path, 'benchmark_jobs'));
  final fileCache = FileCache(directory: cacheDir);
  await fileCache.initialize();
  
  final jobQueue = JobQueue(
    maxConcurrentJobs: 4,
    persistenceCache: fileCache,
    enablePersistence: false, // Disabled for benchmark
  );
  
  const jobCount = 100;
  final completedJobs = <String>{};
  final completer = Completer<void>();
  
  // Listen for completion
  jobQueue.onJobCompleted.listen((result) {
    completedJobs.add(result.jobId);
    
    if (completedJobs.length == jobCount) {
      completer.complete();
    }
  });
  
  // Generate benchmark jobs
  final jobs = List.generate(jobCount, (i) => DocumentProcessingJob(
    id: 'benchmark_job_$i',
    data: {'file_path': '/benchmark/file_$i.txt', 'processing_type': 'benchmark'},
    priority: JobPriority.normal,
  ));
  
  print('üìã Processing $jobCount jobs...');
  final benchmarkStart = DateTime.now();
  
  // Add all jobs
  for (final job in jobs) {
    await jobQueue.addJob(job);
  }
  
  // Wait for completion
  await completer.future.timeout(const Duration(minutes: 2));
  
  final benchmarkTime = DateTime.now().difference(benchmarkStart);
  final throughput = (jobCount / benchmarkTime.inSeconds).toStringAsFixed(1);
  
  print('‚úÖ Processed $jobCount jobs in ${benchmarkTime.inSeconds}s');
  print('üìä Throughput: $throughput jobs/sec');
  
  final stats = jobQueue.getStatistics();
  print('üìä Success rate: ${(stats.successRate * 100).toStringAsFixed(1)}%');
  
  await jobQueue.dispose();
}

/// Mock OpenAI provider for testing
class MockOpenAIProvider implements ModelProvider {
  @override
  String get id => 'mock_openai';
  
  @override
  String get name => 'Mock OpenAI Provider';
  
  @override
  ModelCapabilities get capabilities => const ModelCapabilities(
    supportsStreaming: true,
    supportsTools: true,
    maxTokens: 4000,
    type: ModelType.chat,
  );
  
  @override
  List<ModelInfo> get availableModels => [
    const ModelInfo(
      id: 'gpt-3.5-turbo',
      name: 'GPT-3.5 Turbo',
      maxTokens: 4000,
      costPer1kTokens: 0.002,
    ),
  ];
  
  @override
  Future<bool> get isAvailable async => true;
  
  @override
  Future<ModelResponse> complete(ModelRequest request) async {
    // Simulate API delay
    await Future.delayed(Duration(milliseconds: 200 + Random().nextInt(300)));
    
    // Generate mock response based on input
    final content = _generateMockResponse(request);
    
    return ModelResponse(
      content: content,
      model: request.model,
      usage: TokenUsage(
        promptTokens: _estimateTokens(request.messages),
        completionTokens: _estimateTokens([Message.assistant(content)]),
        totalTokens: _estimateTokens(request.messages) + _estimateTokens([Message.assistant(content)]),
      ),
      finishReason: 'stop',
      metadata: {'mock': true, 'timestamp': DateTime.now().toIso8601String()},
    );
  }
  
  @override
  Stream<String> stream(ModelRequest request) async* {
    final response = await complete(request);
    final words = response.content.split(' ');
    
    for (final word in words) {
      await Future.delayed(const Duration(milliseconds: 50));
      yield '$word ';
    }
  }
  
  @override
  Future<List<ModelInfo>> listModels() async => availableModels;
  
  @override
  Future<ProviderHealth> checkHealth() async {
    return const ProviderHealth(
      providerId: 'mock_openai',
      isHealthy: true,
      latency: Duration(milliseconds: 250),
      errorRate: 0.01,
      metadata: {'mock': true},
    );
  }
  
  String _generateMockResponse(ModelRequest request) {
    final userMessage = request.messages.last.content.toLowerCase();
    
    if (userMessage.contains('capital') && userMessage.contains('france')) {
      return 'The capital of France is Paris. It is located in the north-central part of the country and is known for its iconic landmarks like the Eiffel Tower and Louvre Museum.';
    } else if (userMessage.contains('quantum computing')) {
      return 'Quantum computing is a type of computation that harnesses quantum mechanical phenomena like superposition and entanglement to process information. Unlike classical computers that use bits (0 or 1), quantum computers use quantum bits or qubits that can exist in multiple states simultaneously, potentially allowing them to solve certain problems much faster than classical computers.';
    } else {
      return 'This is a mock response generated for testing purposes. The actual response would depend on the specific model and input provided.';
    }
  }
  
  int _estimateTokens(List<Message> messages) {
    final totalChars = messages.fold<int>(0, (sum, msg) => sum + msg.content.length);
    return (totalChars / 4).ceil(); // Rough approximation
  }
}

/// Test checklist verification
void verifyPerformanceOptimizationTestChecklist() {
  print('\nüß™ PERFORMANCE OPTIMIZATION TEST CHECKLIST');
  print('=' * 50);
  
  final checklist = [
    '‚úÖ Multi-level cache system (L1 Memory, L2 Redis, L3 Disk)',
    '‚úÖ LRU eviction policy with TTL support',
    '‚úÖ Cache hit/miss statistics and monitoring',
    '‚úÖ Background job queue with priority support',
    '‚úÖ Worker pool with auto-scaling',
    '‚úÖ Job persistence and recovery with WAL',
    '‚úÖ Response caching for model providers',
    '‚úÖ Performance benchmarking and metrics',
    '‚úÖ Error handling and graceful degradation',
    '‚úÖ Cleanup and resource management',
  ];
  
  for (final item in checklist) {
    print(item);
  }
  
  print('\n‚úÖ All performance optimization features implemented and tested!');
}